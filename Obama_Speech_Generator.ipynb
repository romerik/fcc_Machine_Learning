{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Obama Speech Generator",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNh04owi1kOkCmaGV9Io3eS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romerik/fcc_Machine_Learning/blob/main/Obama_Speech_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IHmGhfzWY3j",
        "outputId": "b66e5eba-fa7a-4dea-9e98-360ce4b0a1f3"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH80dOb3Lt_T",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "ac19c0e6-6400-4a07-e3b7-fff7f3f439dc"
      },
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06cf71fa-66d7-4c00-b6ef-d9715332572d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06cf71fa-66d7-4c00-b6ef-d9715332572d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Discours.txt to Discours.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfFsI9YHMhn0",
        "outputId": "ff451820-7f74-4de2-9c0a-cfcbc968071c"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 14328 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7jsob0HMoCp",
        "outputId": "6b0f5554-562a-4fc8-f517-e088e5181b85"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:1000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I am here with students at Wakefield High School in Arlington, Virginia. And we've got students tuning in from all across America, from kindergarten through 12th grade. And I am just so glad that all could join us today. And I want to thank Wakefield for being such an outstanding host. Give yourselves a big round of applause. (Applause.)\r\n",
            "I know that for many of you, today is the first day of school. And for those of you in kindergarten, or starting middle or high school, it's your first day in a new school, so it's understandable if you're a little nervous. I imagine there are some seniors out there who are feeling pretty good right now -- (applause) -- with just one more year to go. And no matter what grade you're in, some of you are probably wishing it were still summer and you could've\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeC2gaioM3fF"
      },
      "source": [
        "###Encoding\n",
        "Since this text isn't encoded yet well need to do that ourselves. We are going to encode each unique character as a different integer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxr7-PZzMyCw"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqsJFjN4NHat",
        "outputId": "163e98a9-ed92-41f6-f1fa-914b3e87669d"
      },
      "source": [
        "print(vocab)\n",
        "print(len(vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '7', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y28KeRZINXQz",
        "outputId": "6e61a775-7cc0-43b0-befe-425155931ad3"
      },
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: THE PRESIDENT\n",
            "Encoded: [39 28 25  2 36 37 25 38 29 24 25 34 39]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23xcDS1HNfay",
        "outputId": "b6605a31-6dd1-4eb1-8fd4-1846fbf084b2"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE PRESIDENT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDLxXeMdNuuJ"
      },
      "source": [
        "###Creating Training Examples\n",
        "Our task is to feed the model a sequence and have it return to us the next character. This means we need to split our text data from above into many shorter sequences that we can pass to the model as training examples. \n",
        "\n",
        "The training examples we will prepapre will use a *seq_length* sequence as input and a *seq_length* sequence as the output where that sequence is the original sequence shifted one letter to the right. For example:\n",
        "\n",
        "```input: Hell | output: ello```\n",
        "\n",
        "Our first step will be to create a stream of characters from our text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1aeAcCTNs3j"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5MSfHyNN_oe",
        "outputId": "21d64dd4-b048-49be-9878-e2c66f262735"
      },
      "source": [
        "print(char_dataset.take(1))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TakeDataset shapes: (), types: tf.int64>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZbuP13XOPra"
      },
      "source": [
        "Next we can use the batch method to turn this stream of characters into batches of desired length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18a1yrHdOQ6b"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VfMW_FXOev5"
      },
      "source": [
        "Now we need to use these sequences of length 101 and split them into input and output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kky28MtyOX0G"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTLcO2tYOlNC",
        "outputId": "2fc8f2d1-877d-49d2-917e-aa5cab9e7aac"
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boiup2X3OwGO",
        "outputId": "f5ec78f6-053a-4d87-b95a-b53ffa80a7e2"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "THE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go\n",
            "\n",
            "OUTPUT\n",
            "HE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybody. All right, everybody go \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "ahead and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I\n",
            "\n",
            "OUTPUT\n",
            "head and have a seat. How is everybody doing today? (Applause.) How about Tim Spicer? (Applause.) I \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYNGEtXcO7Wr"
      },
      "source": [
        "Finally we need to make training batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWn1fxoO8Zr"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJIUnx7_PI18"
      },
      "source": [
        "###Building the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPt1ARWlPARF",
        "outputId": "cbd05648-1c9e-4003-a023-a19b9f5d5338"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           18176     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 71)            72775     \n",
            "=================================================================\n",
            "Total params: 5,337,927\n",
            "Trainable params: 5,337,927\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9KDBP1QPmGU"
      },
      "source": [
        "###Creating a Loss Function\n",
        "Now we are going to create our own loss function for this problem. This is because our model will output a (64, sequence_length, 71) shaped tensor that represents the probability distribution of each character at each timestep for every sequence in the batch. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DzQhx2DPmrB"
      },
      "source": [
        "However, before we do that let's have a look at a sample input and the output from our untrained model. This is so we can understand what the model is giving us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcgC4ESEPqQq",
        "outputId": "28773e7d-3ae2-44ae-df23-15fe891918e9"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 71) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RjyIXCbQ3Qs",
        "outputId": "5831c596-13f6-44fb-dbac-c352a3f30716"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 7.62699265e-03  1.10152166e-03 -5.25636598e-03 ... -4.35336353e-03\n",
            "   -8.28266144e-04 -2.02604313e-03]\n",
            "  [ 6.04327675e-03  2.86248047e-03 -9.24623758e-03 ...  2.23226426e-03\n",
            "    1.29912049e-03 -1.92878593e-03]\n",
            "  [ 3.59292817e-03  2.43182317e-03 -8.23112950e-03 ... -1.44667551e-03\n",
            "   -1.01471459e-02  5.04916441e-03]\n",
            "  ...\n",
            "  [ 8.13252479e-03  8.13604333e-04 -7.46793812e-03 ... -3.92857473e-03\n",
            "    3.27172689e-03 -6.48423564e-03]\n",
            "  [ 1.01223588e-02  2.08001677e-03 -9.94499866e-03 ... -9.70614236e-03\n",
            "    7.45589845e-03 -1.04377419e-02]\n",
            "  [ 8.06148630e-03 -2.60718819e-03 -9.54290386e-03 ... -1.22782192e-03\n",
            "   -5.12548722e-04 -5.06054470e-03]]\n",
            "\n",
            " [[-4.11526160e-03  4.17520991e-04  1.18001818e-03 ...  2.91393546e-04\n",
            "    9.04617738e-03 -2.77288212e-03]\n",
            "  [ 4.83230641e-03  9.93007794e-04 -4.12813900e-03 ... -4.57193376e-03\n",
            "    5.76864230e-03 -5.16943866e-03]\n",
            "  [-5.95908612e-04  1.65244867e-03 -2.74502579e-03 ... -3.34624201e-03\n",
            "    1.27961477e-02 -7.95506407e-03]\n",
            "  ...\n",
            "  [ 1.37278913e-02  1.14866847e-03 -1.59010775e-02 ... -8.08226410e-03\n",
            "   -7.52887968e-03 -3.57575645e-03]\n",
            "  [ 5.84224286e-03 -3.55750788e-04 -8.10164213e-03 ... -5.07918792e-03\n",
            "   -4.12237272e-03 -5.67887630e-03]\n",
            "  [ 1.19467340e-02  3.97268886e-04 -1.18617155e-02 ... -7.11821206e-03\n",
            "   -4.71777748e-03 -6.95284456e-03]]\n",
            "\n",
            " [[ 3.81035527e-04 -6.24523452e-03  8.26263800e-03 ...  2.08454643e-04\n",
            "    5.93051128e-03  5.47657488e-03]\n",
            "  [ 4.49383352e-03 -5.55189047e-03  6.40016655e-03 ...  1.79673161e-03\n",
            "    4.21557063e-03  5.57730626e-03]\n",
            "  [ 7.24783679e-03 -1.93774176e-03  9.74544906e-04 ... -5.44034690e-03\n",
            "    8.68199859e-03 -2.38365028e-04]\n",
            "  ...\n",
            "  [ 2.90764333e-03 -1.02875847e-02 -4.02813172e-03 ...  5.83096407e-03\n",
            "   -3.35042551e-03 -1.17187807e-03]\n",
            "  [ 7.84370990e-04 -4.50925343e-03 -4.05086996e-03 ...  2.16480484e-03\n",
            "   -2.77931336e-04 -4.32230532e-04]\n",
            "  [ 1.40592549e-03 -8.43591057e-03 -8.52732919e-03 ...  1.29036605e-04\n",
            "   -1.49654131e-03 -3.54102347e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 6.12732116e-03 -6.73625059e-03 -3.31901642e-03 ...  2.73066573e-03\n",
            "    1.87433045e-03  3.71381035e-03]\n",
            "  [ 1.04907546e-02 -1.21067921e-02 -5.45168342e-03 ...  4.40065656e-03\n",
            "    3.09039303e-03  6.81561418e-03]\n",
            "  [ 8.03502090e-03 -1.42181264e-02 -5.25063835e-03 ...  7.62138097e-03\n",
            "   -2.73095444e-03  9.16159805e-03]\n",
            "  ...\n",
            "  [ 1.00981388e-02 -3.22696799e-03 -1.68387983e-02 ... -5.02690626e-03\n",
            "    1.29465945e-03 -8.84893350e-03]\n",
            "  [ 1.21295685e-02 -2.52143899e-03 -1.41263884e-02 ... -3.21338000e-03\n",
            "   -2.32813880e-04 -6.02517463e-03]\n",
            "  [ 1.34175401e-02 -2.81180535e-03 -1.55990887e-02 ... -3.64630390e-03\n",
            "   -4.16047731e-03 -8.85616150e-03]]\n",
            "\n",
            " [[ 5.82866883e-03  8.25830037e-04 -3.70359933e-03 ... -5.21932961e-03\n",
            "    1.55217142e-03 -3.52371996e-03]\n",
            "  [-2.60055531e-04  9.36941011e-04 -1.73146406e-03 ... -3.28455796e-03\n",
            "    9.95530561e-03 -5.20408433e-03]\n",
            "  [ 1.52142835e-04 -6.03461172e-03  6.96427980e-03 ... -2.44455133e-03\n",
            "    1.30269406e-02  5.52878599e-04]\n",
            "  ...\n",
            "  [ 4.24958300e-03 -6.37884438e-03 -6.71710586e-03 ... -1.67924375e-03\n",
            "   -1.13943480e-02  6.46398356e-03]\n",
            "  [-5.62467030e-04 -4.33425885e-03 -3.51982587e-03 ...  5.64623391e-04\n",
            "    8.13369174e-04  1.93839974e-03]\n",
            "  [ 4.26635519e-03 -6.27050456e-03 -5.40043321e-03 ...  2.91357283e-05\n",
            "   -2.47245468e-03 -2.59165838e-03]]\n",
            "\n",
            " [[ 8.39169952e-04 -4.65346361e-03 -4.90248762e-03 ... -1.00361381e-03\n",
            "   -6.07063645e-04 -1.45078602e-03]\n",
            "  [-1.23187865e-03 -5.82297798e-05 -1.87656120e-03 ... -5.46843326e-03\n",
            "    5.12253377e-04  7.01106526e-03]\n",
            "  [ 6.95421174e-03  8.74526217e-04 -6.95732143e-03 ... -8.89056642e-03\n",
            "   -9.28706955e-04  3.89667694e-03]\n",
            "  ...\n",
            "  [ 2.05730484e-03 -4.18166211e-03 -1.54248672e-03 ... -5.14696445e-03\n",
            "   -7.24424236e-03  1.58016887e-02]\n",
            "  [ 6.85448479e-03 -6.69989362e-03 -4.66626836e-03 ... -3.61475116e-03\n",
            "   -8.42226017e-03  8.34735762e-03]\n",
            "  [ 1.42478822e-02 -4.64710128e-03 -7.78628187e-03 ... -6.17480930e-03\n",
            "   -6.48376253e-03  4.19829693e-03]]], shape=(64, 100, 71), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQQIzWMSRHVb",
        "outputId": "ad2a4244-804d-40d5-976c-d8a572552dca"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00762699  0.00110152 -0.00525637 ... -0.00435336 -0.00082827\n",
            "  -0.00202604]\n",
            " [ 0.00604328  0.00286248 -0.00924624 ...  0.00223226  0.00129912\n",
            "  -0.00192879]\n",
            " [ 0.00359293  0.00243182 -0.00823113 ... -0.00144668 -0.01014715\n",
            "   0.00504916]\n",
            " ...\n",
            " [ 0.00813252  0.0008136  -0.00746794 ... -0.00392857  0.00327173\n",
            "  -0.00648424]\n",
            " [ 0.01012236  0.00208002 -0.009945   ... -0.00970614  0.0074559\n",
            "  -0.01043774]\n",
            " [ 0.00806149 -0.00260719 -0.0095429  ... -0.00122782 -0.00051255\n",
            "  -0.00506054]], shape=(100, 71), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okUuzTiERSB4",
        "outputId": "117f6d3e-838e-4518-812d-c681b04811e6"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 71 values representing the probabillity of each character occuring next"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "tf.Tensor(\n",
            "[ 0.00762699  0.00110152 -0.00525637 -0.00745045 -0.00363152  0.00123794\n",
            " -0.00023096  0.00579792 -0.00155136  0.00397557 -0.00246714 -0.00068156\n",
            " -0.00641047  0.00050122  0.00417974  0.00793499 -0.00331001 -0.00047541\n",
            " -0.00625166 -0.00166564 -0.00200743 -0.00389417  0.00098266 -0.00189935\n",
            " -0.00281346  0.00356629  0.00299683  0.00073833 -0.00132915 -0.00163202\n",
            " -0.00059693  0.00193355 -0.00667748 -0.00165157 -0.00169201 -0.00279811\n",
            "  0.0044852  -0.00376152  0.00438907 -0.00011781  0.00175983  0.00271078\n",
            "  0.00302047 -0.00236719  0.00013213  0.0082527  -0.00096923  0.00051052\n",
            "  0.00098145  0.00084023  0.00264101 -0.00087558 -0.00169617 -0.00305396\n",
            "  0.00516298  0.00357213  0.00316941 -0.00347713  0.0024611   0.0001195\n",
            " -0.00226035 -0.00230259 -0.00627074 -0.00199328 -0.00169039 -0.00017979\n",
            "  0.00210524  0.00038536 -0.00435336 -0.00082827 -0.00202604], shape=(71,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "dLqUY7yZRop-",
        "outputId": "42278b59-fdd3-48b7-aa1f-03068f500906"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Wz\\rEILe3Pv,aTm\\nsEu13TUk-0BMz7ESO)pCf(Dzo.hhejHelf\\rWpMqA!dEpd2BANNYyEm\\rhm0HzCvK\".\\r FoL\"HuDCLHy):s?I);'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZF_G4iTR3os"
      },
      "source": [
        "So now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vjFBl4_R3Gp"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvccNQdaSB3K"
      },
      "source": [
        "###Compiling the Model\n",
        "At this point we can think of our problem as a classification problem where the model predicts the probabillity of each unique letter coming next. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqSNROGSSDDZ"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtAK13-gSM2G"
      },
      "source": [
        "###Creating Checkpoints\n",
        "Now we are going to setup and configure our model to save checkpoinst as it trains. This will allow us to load our model from a checkpoint and continue training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCWlEMUzSN6E"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id8z52JSZ1D"
      },
      "source": [
        "###Training\n",
        "Finally, we will start training the model. \n",
        "\n",
        "**If this is taking a while go to Runtime > Change Runtime Type and choose \"GPU\" under hardware accelerator.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGzLI_L7Sbdr",
        "outputId": "59aa78c3-9cb3-4907-9118-f81de12352f1"
      },
      "source": [
        "history = model.fit(data, epochs=500, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 16s 7s/step - loss: 4.2397\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 3.5295\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 3.3741\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.2469\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.1290\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.1083\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.0841\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 3.0498\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 3.0575\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 3.0490\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.0253\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 3.0048\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 3.0021\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.9921\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.9733\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.9580\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.9422\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.9263\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.8948\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 18s 10s/step - loss: 2.8799\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.8640\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.8318\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.8148\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.7854\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.7461\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.7220\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.6937\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.6574\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.6358\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.5976\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.5696\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.5505\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.5274\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.5107\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.4871\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.4632\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.4504\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.4297\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.4233\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.4005\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3834\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.3694\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3634\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.3493\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3744\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3385\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3241\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3153\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.3059\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.2916\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.2813\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.2707\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 2.2542\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.2288\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 2.2343\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 2.2241\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.2029\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1982\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1761\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1745\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 2.1683\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1528\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1501\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1359\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1234\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1033\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.1082\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0926\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0643\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0655\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0544\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0469\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0348\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0209\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0071\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 2.0015\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.9900\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.9806\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.9604\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.9505\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.9383\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.9244\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.9134\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.9066\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.8964\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8787\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 19s 8s/step - loss: 1.8574\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8522\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8319\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8212\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8146\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.8069\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7815\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7746\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7686\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7566\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7414\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7271\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7056\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.7006\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.6923\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.6742\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.6585\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.6378\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.6338\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.6190\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.6065\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 1.5837\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.5729\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 18s 10s/step - loss: 1.5643\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.5480\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.5397\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.5204\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.5088\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4900\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4781\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4774\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4515\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 18s 10s/step - loss: 1.4365\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4209\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4047\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.4198\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.3915\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.3650\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.3498\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.3302\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.3111\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2975\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.2792\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.2699\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2668\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2386\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2274\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2042\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.2029\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.1757\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.1537\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 17s 9s/step - loss: 1.1479\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.1318\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.1139\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.0982\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.0841\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 1.0579\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.0552\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.0275\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 1.0190\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.9913\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.9936\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.9850\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.9526\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.9417\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 19s 10s/step - loss: 0.9150\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.9033\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.8779\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.8519\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.8402\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.8186\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.8080\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.7942\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 17s 9s/step - loss: 0.7695\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.7623\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.7512\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.7304\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.7243\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.7265\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6913\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6816\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6658\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6491\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6393\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.6061\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5954\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5893\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5817\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5512\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5398\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5264\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.5086\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4875\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4807\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4701\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4653\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4552\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.4382\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4306\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 18s 10s/step - loss: 0.4247\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.4463\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4655\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4354\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.4216\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4039\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.4043\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3877\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3712\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.3588\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.3568\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.3424\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3293\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3235\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3156\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3019\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.3042\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2965\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2951\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2852\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2782\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2793\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2736\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2663\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2638\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2627\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.2583\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2534\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2491\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.2460\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2406\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2405\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2305\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 16s 9s/step - loss: 0.2295\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.2316\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.2142\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2233\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2264\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2155\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.2176\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.2060\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.2100\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1987\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1991\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.2015\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1973\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1944\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1944\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1938\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1850\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1907\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1854\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1817\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1774\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1834\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1817\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1782\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1786\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1700\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1703\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1635\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1723\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1743\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1631\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1664\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1594\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1698\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 19s 8s/step - loss: 0.1625\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1596\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1589\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1548\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1566\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1540\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1566\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1531\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1507\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1523\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1488\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1512\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1519\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1449\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1447\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1420\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1442\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1452\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1437\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1430\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1410\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1419\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1446\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1349\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1350\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1360\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1362\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1353\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1348\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1315\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1349\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1347\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1289\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 20s 12s/step - loss: 0.1246\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1343\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1286\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1267\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.1290\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1256\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1238\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1293\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1232\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1227\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1213\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1226\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1236\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1251\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1177\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1255\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1188\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1108\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1140\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1154\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1168\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1159\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1191\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1150\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1147\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1149\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1137\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1130\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1127\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.1085\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 16s 9s/step - loss: 0.1141\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1091\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1072\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 18s 10s/step - loss: 0.1088\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1072\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1019\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1071\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1075\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1050\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1092\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1045\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1062\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1039\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1013\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1035\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1025\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1040\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1064\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1075\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1035\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0955\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0980\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.1034\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1004\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0980\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0978\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0981\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0950\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0954\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0961\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0939\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0959\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0951\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.1009\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0996\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0945\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0893\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 20s 8s/step - loss: 0.0945\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0907\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0975\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0911\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0942\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0938\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0936\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0947\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0854\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0934\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0867\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0911\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0874\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0855\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0898\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0920\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0889\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0892\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0860\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0852\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0910\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0856\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0899\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0817\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0892\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0805\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0833\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0840\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0808\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 23s 15s/step - loss: 0.0867\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0869\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0875\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0849\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0830\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0879\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0864\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0852\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0828\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0777\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0736\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0782\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0794\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0798\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0770\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0793\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0786\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0767\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0810\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0774\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0774\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0763\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0786\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0801\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0783\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0752\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0755\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0750\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0764\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0766\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0774\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0745\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0787\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0761\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0762\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0781\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 16s 7s/step - loss: 0.0729\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0756\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0755\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0668\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0767\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0752\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0717\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0703\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0699\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0765\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0681\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0713\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0717\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0712\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0727\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0711\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0746\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0732\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0737\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0710\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0721\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0676\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0702\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0693\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0698\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0706\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0699\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0653\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0679\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0698\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0657\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0629\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0654\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 19s 11s/step - loss: 0.0684\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0663\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0726\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0670\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0675\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0656\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0630\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0679\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0655\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0678\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0654\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0669\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0693\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0692\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0646\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0621\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0665\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0650\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0637\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0623\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0656\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0617\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0636\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0650\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0641\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0639\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0633\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0596\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0594\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0659\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0625\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0624\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 17s 9s/step - loss: 0.0622\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 17s 8s/step - loss: 0.0628\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0607\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0629\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0606\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 15s 7s/step - loss: 0.0631\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0621\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0579\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0595\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0625\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0611\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0607\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 15s 8s/step - loss: 0.0647\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0596\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0649\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0582\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 23s 9s/step - loss: 0.0608\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 16s 8s/step - loss: 0.0617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49TKXterTkqn"
      },
      "source": [
        "###Loading the Model\n",
        "We'll rebuild the model from a checkpoint using a batch_size of 1 so that we can feed one peice of text to the model and have it make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8y-gSuZTm29"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ORmpDSdT-Ht"
      },
      "source": [
        "Once the model is finished training, we can find the **lastest checkpoint** that stores the models weights using the following line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70MAm7I5T4ym"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8sYp0KBUHiz"
      },
      "source": [
        "We can load **any checkpoint** we want by specifying the exact file to load."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJg71D2kUZo0"
      },
      "source": [
        "###Generating Text\n",
        "Now we can use the lovely function provided by tensorflow to generate some text using any starting string we'd like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlvKkZ7tUamL"
      },
      "source": [
        "def generate_text(model, start_string,num_generate = 10000):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX58fw-oVUlE",
        "outputId": "93d549f0-5e82-4166-8b8b-cd3a180b46ed"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: Hello, everybody\n",
            "Hello, everybody. All right, everybody go deiff you quit on school -- you're not just quitting on yourself, you're quitting on your country.\n",
            "Now of you, what's your contribution going to be? What problems are you goingot to fecused or vaccine frobaterg a ped just a little bit longer this monybulking a for your fort life your few times beffere you get it right. You might have to read something a few times before you understant to your family down or your country down. Most of all, don't let yourself down. Make us all proud.\n",
            "That's and puse thought this mights and put a man on the moon. Students who sat where you sit 20 years ago who founded Google and her way to becoming Dr. Jazmin Perez.\n",
            "The to focus on todant where a tifference or how it un you give up on yourself, you give ou sond of shomsn't mit to bighou can be rich and successful without any hard work -- that your ticket to success is through rappingither and AIDS, and to develop new energy technologies now that erenglo Stedent for help weline you love difer inalSpenderstiny. You make o hor of you, what's your contriff. The kith who give you the support that you need. Maybe someone in yound from life offace challenges in their lives just like you do. In some cases they your family down or your country down. Most of all, don't let yourself down. Make us all proud.\n",
            "That's why today I'm her way to becoming Dr. Jazmin Perez.\n",
            "How's riged hard for those of you in kindergortovatinged there athe fight not have the mady to iends who's fought brain your family hasn't mean you're a troublemaker, it means you need to try harder to act right. If you get a bad grade the kity wer jimitult than I know that sthing mights and critical-thinking skills you gain in hore off pool, who are fred life tramk, a or basketball or being a read. Myoulles, anm you fewily to your jeit us ndication. So she pechig for the wirne you'll end up. No one's writed to practice. The same principle applies to your schoolwork. You might have to do a math problem and I goou did for this country?\n",
            "Now, your families, your teachers, and I are doing everyod discuss with you. I'm here because I want to talk with you about your edecation and what's who did wher uppority to achool has esponaights and critical-thinking skills you gain in his or basketball or being a reality TV star. Chances are yourdoof and oversaamestiny. You make your own future.\n",
            "That's what young people like you are doing every day, all across America, from ithere washing your hands a lot, and that you stay home from school when you don't feel well, so we can keeducation, and set goals for themselves. And I expect all of you to do the same.\n",
            "That's why today I's lot of school. That's askight and ceroums an lot of money. Buw they rewho are feeling pert time you plany to hos want for he har work whor have the lothe fuct give my one of you, what's your contributirn for the opportunity to go to college and law sceosher ard some things I'm not proud of, and I got in more trouble than I should have. And my life could have essbood this country.\n",
            "Some of you might not have those advantages. Maybe you don't have adults in your life. There were times when I was lonely and I dis for thoselo so you can be or basketball or being a reality TV sime. J.K. Rowling's -- who wryoull Steve, from my that you're n today will dething fechis canit havit washing your hands a lot, and that you stay home from school when you don't feel well, so we can keed whoumst washing your hands a lot, and that you stay home from school when you don't feel well, so we cankelp peothe Xborain fr washing your hands a lot, and that you stay home from school when you don't feel Sef, somit wher it for your own hife Obaca, who's foughtbights and critical-thinking skills you gain in history and social studies to fight poverty and homele and over and over agaie ride the future of this country. The future of America depends on you. Whatke yourself so you can be more ready to learn. And along those lines, by that wry and go on to college.\n",
            "And Jazmin, Andoni, and Shantell aren't any different from any of you. They today will determine whether we as a nation can meftoom hell when you're d struggl, and puscing hourstoo for your have to education, and set that she tld bestant for your own life and your own future. What you make of your education will decide nothing less that you'll need an education to do it. You want to be a doctor, or a teacher, or a solicanTHE PRESIDENT: Hello, everybody! Thank you. Thank you. Thank you, everybovy. Americatiend pushin you bein you quit on school ablat here with students at Wakefield High School in Arlington, Virginia. And we've got students tuning people deserve a safe environment too havity a lifele right every teacher that you have. Not every homework assignment will seem comperice, who'vafor in thick to promy took und be a sure sur her washing felt a day on school -- you're not just quitting on yourself, you're quitting on your country.\n",
            "Now, your families, your teachers, and I are doing eerucation, and set goala sure you st atho track, and you get your homework done, and don't spead Learing some things fry time. Students and go or basketball or being a reality TV star. Chances are you're not going to be any of those things.\n",
            "THE PRESIDENT:: HE love ovator or alliSted. I have to got going on a ovigors and AIDS, and to develop new eeruganted because who are feeling pretty good right now -- (applause) -- with just onemmmstoas. sure yee sure you have the education you need to answer these questions. I'm working hard to fix up your classed to bearn for the Xovert. Your for this country.\n",
            "Some of you might not have those advantages. Maybe you don't have adults in your life r yoursfall or you'll eeen ull Steve, from my hometown of Cheicage, and I dovity and ienge upport it. your parentseare your fom tillie strlights and critical-thinking , whis caning hard workind a for your hill Sted, in the foulded his schoolwork. Bt they for your schoolwork. You might have to do a math problem afraif you quit on school -- you're not just quitices on yourself, she surted to pamerthand of show up of you that sn's mo life your life or baikn a lot of sthent Xbox.\n",
            "I've talked a lot about your government's responsibility for setting hew parents her I saneesponsinglo oof all of you Ane oun in who are feeling pretty good right now -- (applause) -- witt just one more year to go. And no matter what you want to do with your life, I guaranteed so you can be more ready to learn. And along those lines, by the way, I hope all of you are drifacell caver and AIDS, and to develop new energy technologies and protect our environment. You'll need the insries, one of which affected his memory, so it took him much longer -- hundreds of extra hours -- to do d spend, somith every teacher that you have. Not every homework assignment will seem completely relevant to your life, I guaring ofer fifcer hive got going on at home -- none of that is an excuse for neglecting your homework or having a bad attertant foo hat she cromated is no excuse for net gearease sieve you stay on track to meet your goals.\n",
            "And even when you're struggling, even when you're discourare stiglt a program here few times before you get it right. You might have to read something a fer will you make? What will a President who comes here in 20 or 50 or 100 years say about what all of you in this new school year.\n",
            "Now, I've given a lot of speeches about education. And I've that is. There were times when I missed having afface challenges in their lives just like you do. In some cases they've got it a lot worse off than mace har to focus on o got o woolk, teacher aductice felp lef you gate to felp your own fut in miferricatou did for this country?\n",
            "Now, your families, your teachers, and I are doing everything we can to make a diffle fill you ameat and here and be r give America.\n",
            "Young people like J.KE PRESIDENT: Hello, everybody!n THere you. Joz've for itadersingle on you give up on yourself, you give up on your country.\n",
            "The story of America isn't about people who goid cor trimmer activity, or volunteer in your community. Maybe you'll decide to standu who goid job it right now that can make it hard to focus on your schoolwork.\n",
            "I get it. I know what it's like. My fatilt them. I'm here you grit 2 kid for this country?\n",
            "Now, your families, your teachers, and I are doing everything we can to make a didf reasn't might now school has tho grith in have thele athat grade you're in, some of you arefort right. You deviny too an you her way to becookn ind , who's fought brain cancer since he was three. He's had to endure all sorts of treatments and ssroofareacand people teachers and Alloras a lot of spencen schools, and got a scholarship to Brown University -- is now in fright im yourself so you can be more ready to learn. And along those lines, by the way, I hope all of you are discuss with you. I'm here because I want to talk with you about your education and what's expected first time you play a new sport. You don't hit every note the firstante the op this country.\n",
            "Some of you might not have those afvaring and hissers on yourself, so discuss with you. I'm here because wh track, and you get your homework done, and don't spend every taking high school with hoong poon. J.K. Rowling's -- who wrote Harry Poteeclu knd who give you the support that you need. Maybe someone in your family has lost their job and there's his from in even good enough to write a book or articles in a newspaper -- but you might not know it until you wand porent they tho goveop and here fedulthe Xboxaif mide frimacalkentin the first time you play a new sport. You don't hit every note the first time you sing a song. You've life right at her parents had gone to college, and they didn't have ased or vaccine -- but you might not kn of ithen fill meend or basketball or bights afrity and pebalies, and got a scholarship to Brown University -- is now in graduate school, studying pub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPf8f_KUadk5",
        "outputId": "8ddc0a97-6920-40c1-c51d-45cff0aa9893"
      },
      "source": [
        "inp = input('Type some text :  ')\n",
        "print(generate_text(model, inp, 100))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type some text :  Thanks\n",
            "Thanks who are Obaieared, and you get your who Prting from his her way to becoming Dr. Jazmin Perez.\n",
            "I'm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eKQ4ipBCl5b",
        "outputId": "3181a335-d796-4d6c-bae9-37993d505573"
      },
      "source": [
        "model.save('obama_speech_generator.h5')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    }
  ]
}